{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAST AI Lesson - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help us in plotting using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Fastai libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.nlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NLP(Natural Language Processing) we are going to work on the IMDB movie view dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the dataset and then we are printing out the files in /train/pos directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/aclImdb/'\n",
    "names = ['neg', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  \u001b[0m\u001b[01;34mpos\u001b[0m/    unsupBow.feat  urls_pos.txt\r\n",
      "\u001b[01;34mneg\u001b[0m/             \u001b[01;34munsup\u001b[0m/  urls_neg.txt   urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {path}train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_9.txt\r\n",
      "10000_8.txt\r\n",
      "10001_10.txt\r\n",
      "10002_7.txt\r\n",
      "10003_8.txt\r\n",
      "10004_8.txt\r\n",
      "10005_7.txt\r\n",
      "10006_7.txt\r\n",
      "10007_7.txt\r\n",
      "10008_7.txt\r\n",
      "ls: write error\r\n"
     ]
    }
   ],
   "source": [
    "%ls {path}train/pos | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts_labels_from_folders() function basically to the directory f'{path}train' and then in that directory go through\n",
    "each file start with words in names list. And then divide it in training and test set and allot the value to it based \n",
    "on names list. Like for the data from neg folder will be given the value 0 and then the data from pos will be given 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, trn_y = texts_labels_from_folders(f'{path}train', names)\n",
    "val, val_y = texts_labels_from_folders(f'{path}test', names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m gettin\\' sick of movies that sound entertaining in a one-line synopsis then end up being equal to what you\\'d find in the bottom center of a compost heap.<br /><br />Who knows: \"Witchery\" may have sounded interesting in a pitch to the studios, even with a \"big name cast\" (like Blair and Hasselhoff - wink-wink, nudge-nudge) and the effervescent likes of Hildegard Knef (I dunno, some woman...).<br /><br />But on film, it just falls apart faster than a papier-mache sculpture in a rainstorm. Seems these unfortunate folks are trapped in an island mansion off the Eastern seaboard, and one of them (a woman, I\\'d guess) is being targeted by a satanic cult to bear the child of hell while the others are offed in grotesque, tortuous ways. <br /><br />Okay, right there you have a cross-section of plots from \"The Exorcist\", \"The Omen\", \"Ten Little Indians\" and a few other lesser movies in the satanic-worshippers-run-amok line. None of it is very entertaining and for the most part, you\\'ll cringe your way from scene to scene until it\\'s over.<br /><br />No, not even Linda Blair and David Hasselhoff help matters much. They\\'re just in it to pick up a paycheck and don\\'t seem very intent on giving it their \"all\". <br /><br />From the looks of it, Hasselhoff probably wishes he were back on the beack with Pam Anderson (and who can blame him?) and Linda... well, who knows; a celebrity PETA benefit or pro-am golf tour or whatever it is she\\'s in to nowadays.<br /><br />And the torture scenes! Ecchhhh. You\\'ll see people get their mouths sewn shut, dangled up inside roaring fireplaces, strung up in trees during a violent storm, vessels bursting out of their necks, etc, etc. Sheesh, and I thought \"Mark of the Devil\" was the most sadistic movie I\\'d seen....<br /><br />Don\\'t bother. It\\'s not worth your time. I can\\'t believe I told you as much as I did. If you do watch it, just see if you can count the cliches. And yes, Blair gets possessed, as if you didn\\'t see THAT coming down Main Street followed by a marching band.<br /><br />No stars. \"Witchery\" - these witches will give you itches.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label for the above text is 0. Means negative because we have neg on 0th index in names list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our job is to take trn and predict trn_y for each text data in testing set.\n",
    "For this we are going to create a document matrix and then we will remove the order of word for this. This Unordered \n",
    "text here is called bag of words which tell us about the words that are in that document.\n",
    "So document matrix is matrix of token counts. For this we are going to do following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function tokenize at 0x7faf7b80bea0>, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_doc = vec.fit_transform(trn)\n",
    "val_doc = vec.transform(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have created a matrix for based on the training data for this we make the use of fit_transform()\n",
    "and then we are transform() function just to create a matrix based on validation data but we are taking the same order\n",
    "of word from the training data document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the dimension for the trn_doc is 25000 which is number of text documents in training set and \n",
    "75132 is the number of different words that we came across these 25000 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3640465 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets grab the first row which for the 1st movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 244 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 244 words in total are used from 75132. Now we are grabbing the some of the words from the 1st text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aussie',\n",
       " 'aussies',\n",
       " 'austen',\n",
       " 'austeniana',\n",
       " 'austens',\n",
       " 'auster',\n",
       " 'austere',\n",
       " 'austerity',\n",
       " 'austin',\n",
       " 'austinese',\n",
       " 'austion',\n",
       " 'austrailan',\n",
       " 'austrailia',\n",
       " 'austrailian',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'australians',\n",
       " 'australlian',\n",
       " 'austreheim',\n",
       " 'austreim',\n",
       " 'austria',\n",
       " 'austrialian',\n",
       " 'austrian',\n",
       " 'austrians',\n",
       " 'austro',\n",
       " 'autant',\n",
       " 'auteil',\n",
       " 'auteuil',\n",
       " 'auteur',\n",
       " 'auteurist',\n",
       " 'auteurs',\n",
       " 'authenic',\n",
       " 'authentic',\n",
       " 'authentically',\n",
       " 'authenticating',\n",
       " 'authenticity',\n",
       " 'author',\n",
       " 'authored',\n",
       " 'authoress',\n",
       " 'authorial',\n",
       " 'authoring',\n",
       " 'authorisation',\n",
       " 'authorised',\n",
       " 'authoritarian',\n",
       " 'authoritarianism',\n",
       " 'authoritative',\n",
       " 'authoritatively',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authorize',\n",
       " 'authorized',\n",
       " 'authorizes',\n",
       " 'authorizing',\n",
       " 'authors',\n",
       " 'authorship',\n",
       " 'authorty',\n",
       " 'authur',\n",
       " 'autie',\n",
       " 'autism',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'autobiograhical',\n",
       " 'autobiographic',\n",
       " 'autobiographical',\n",
       " 'autobiography',\n",
       " 'autobots',\n",
       " 'autocockers',\n",
       " 'autocracy',\n",
       " 'autocratic',\n",
       " 'autocue',\n",
       " 'autofocus',\n",
       " 'autograph',\n",
       " 'autographed',\n",
       " 'autographs',\n",
       " 'automag',\n",
       " 'automaker',\n",
       " 'automakers',\n",
       " 'automata',\n",
       " 'automated',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'automatics',\n",
       " 'automation',\n",
       " 'automaton',\n",
       " 'automatons',\n",
       " 'automobile',\n",
       " 'automobiles',\n",
       " 'automotive',\n",
       " 'autonomous',\n",
       " 'autonomy',\n",
       " 'autons',\n",
       " 'autopilot',\n",
       " 'autopsied',\n",
       " 'autopsies',\n",
       " 'autopsy',\n",
       " 'autorenfilm',\n",
       " 'autos',\n",
       " 'autre',\n",
       " 'autry',\n",
       " 'autumn',\n",
       " 'autumnal',\n",
       " 'aux',\n",
       " 'auxiliaries',\n",
       " 'auxiliary',\n",
       " 'auzzie',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'avail',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'avails',\n",
       " 'avalanche',\n",
       " 'avalanches',\n",
       " 'avalon',\n",
       " 'avalos',\n",
       " 'avant',\n",
       " 'avante',\n",
       " 'avantegardistic',\n",
       " 'avantgarde',\n",
       " 'avantguard',\n",
       " 'avanti',\n",
       " 'avarice',\n",
       " 'avaricious',\n",
       " 'avariciously',\n",
       " 'avast',\n",
       " 'avatar',\n",
       " 'avatars',\n",
       " 'avati',\n",
       " 'avco',\n",
       " 'ave',\n",
       " 'avec',\n",
       " 'avenet',\n",
       " 'avenge',\n",
       " 'avenged',\n",
       " 'avenger',\n",
       " 'avengers',\n",
       " 'avenges',\n",
       " 'avenging',\n",
       " 'avent',\n",
       " 'aventura',\n",
       " 'aventurera',\n",
       " 'avenue',\n",
       " 'avenues',\n",
       " 'aver',\n",
       " 'average',\n",
       " 'averaged',\n",
       " 'averagely',\n",
       " 'averages',\n",
       " 'averagey',\n",
       " 'averaging',\n",
       " 'averagousity',\n",
       " 'averback',\n",
       " 'averse',\n",
       " 'aversion',\n",
       " 'avert',\n",
       " 'averted',\n",
       " 'avery',\n",
       " 'aveu',\n",
       " 'avg',\n",
       " 'avi',\n",
       " 'avian',\n",
       " 'aviation',\n",
       " 'aviator',\n",
       " 'aviatrix',\n",
       " 'avid',\n",
       " 'avidly',\n",
       " 'avigail',\n",
       " 'avigdor',\n",
       " 'avignon',\n",
       " 'avin',\n",
       " 'aving',\n",
       " 'avionics',\n",
       " 'aviv',\n",
       " 'avjo',\n",
       " 'avni',\n",
       " 'avocado',\n",
       " 'avocation',\n",
       " 'avoid',\n",
       " 'avoidable',\n",
       " 'avoidance',\n",
       " 'avoidances',\n",
       " 'avoide',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'avon',\n",
       " 'avonlea',\n",
       " 'avowed',\n",
       " 'avowedly',\n",
       " 'avril',\n",
       " 'avro',\n",
       " 'avsar',\n",
       " 'avtar',\n",
       " 'avuncular',\n",
       " 'avventura',\n",
       " 'avy',\n",
       " 'aw',\n",
       " 'awa',\n",
       " 'awaaaaay',\n",
       " 'await',\n",
       " 'awaited',\n",
       " 'awaiting',\n",
       " 'awaits',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'awakened',\n",
       " 'awakening',\n",
       " 'awakeningly',\n",
       " 'awakenings',\n",
       " 'awakens',\n",
       " 'awakes',\n",
       " 'awaking',\n",
       " 'awara',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'awardees',\n",
       " 'awarding',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'awash',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awe',\n",
       " 'awed',\n",
       " 'aweful',\n",
       " 'awefully',\n",
       " 'aweigh',\n",
       " 'awes',\n",
       " 'awesome',\n",
       " 'awesomely',\n",
       " 'awesomenes',\n",
       " 'awesomeness',\n",
       " 'awestruck',\n",
       " 'awful',\n",
       " 'awfull',\n",
       " 'awfully',\n",
       " 'awfulness',\n",
       " 'awfuwwy',\n",
       " 'awhile',\n",
       " 'awkrawrd',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'awkwardness',\n",
       " 'awlright',\n",
       " 'awoke',\n",
       " 'awoken',\n",
       " 'awol',\n",
       " 'awry',\n",
       " 'awsome',\n",
       " 'awsomeness',\n",
       " 'awstruck',\n",
       " 'awtwb',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'awwwww',\n",
       " 'awwwwww',\n",
       " 'ax',\n",
       " 'axe',\n",
       " 'axed',\n",
       " 'axel',\n",
       " 'axellent',\n",
       " 'axes',\n",
       " 'axiom',\n",
       " 'axiomatic',\n",
       " 'axis',\n",
       " 'axl',\n",
       " 'axton',\n",
       " 'aya',\n",
       " 'ayacoatl',\n",
       " 'ayat',\n",
       " 'ayatollah',\n",
       " 'ayatollahs',\n",
       " 'ayats',\n",
       " 'ayda',\n",
       " 'aye',\n",
       " 'ayer',\n",
       " 'ayers',\n",
       " 'ayesh',\n",
       " 'ayesha',\n",
       " 'ayin',\n",
       " 'aykroyd',\n",
       " 'aylmer',\n",
       " 'aymeric',\n",
       " 'aymler',\n",
       " 'ayn',\n",
       " 'ayone',\n",
       " 'ayre',\n",
       " 'ayres',\n",
       " 'ayurvedic',\n",
       " 'ayutthaya',\n",
       " 'az',\n",
       " 'azadi',\n",
       " 'azam',\n",
       " 'azar',\n",
       " 'azaria',\n",
       " 'azariah',\n",
       " 'azema',\n",
       " 'azghar',\n",
       " 'azimov',\n",
       " 'azjazz',\n",
       " 'azkaban',\n",
       " 'azmi',\n",
       " 'azn',\n",
       " 'aznable',\n",
       " 'aznar',\n",
       " 'azoic',\n",
       " 'azoids',\n",
       " 'aztec',\n",
       " 'azteca',\n",
       " 'aztecs',\n",
       " 'azuma',\n",
       " 'azumi',\n",
       " 'azur',\n",
       " 'azusagawa',\n",
       " 'aïssa',\n",
       " 'b',\n",
       " 'b2',\n",
       " 'b36s',\n",
       " 'b4',\n",
       " 'b5',\n",
       " 'b\\\\c',\n",
       " 'ba',\n",
       " 'baaaaaaaaaaaaaad',\n",
       " 'baaaaaaaaaaad',\n",
       " 'baaaaaaaaaad',\n",
       " 'baaaaaad',\n",
       " 'baaaaad',\n",
       " 'baaaad',\n",
       " 'baaad',\n",
       " 'baad',\n",
       " 'baal',\n",
       " 'baap',\n",
       " 'baas',\n",
       " 'baastard',\n",
       " 'baba',\n",
       " 'babaganoosh',\n",
       " 'babaji',\n",
       " 'babaloo',\n",
       " 'babar',\n",
       " 'babbage',\n",
       " 'babban',\n",
       " 'babbette',\n",
       " 'babbitt',\n",
       " 'babble',\n",
       " 'babbles',\n",
       " 'babbling',\n",
       " 'babbs',\n",
       " 'babcock',\n",
       " 'babe',\n",
       " 'babel',\n",
       " 'babelfish',\n",
       " 'babes',\n",
       " 'babesti',\n",
       " 'babette',\n",
       " 'babhi',\n",
       " 'babied',\n",
       " 'babies',\n",
       " 'babified',\n",
       " 'babitch',\n",
       " 'baboon',\n",
       " 'baboons',\n",
       " 'baboushka',\n",
       " 'babs',\n",
       " 'babson',\n",
       " 'babtise',\n",
       " 'babu',\n",
       " 'babushkas',\n",
       " 'baby',\n",
       " 'babyface',\n",
       " 'babyhood',\n",
       " 'babylon',\n",
       " 'babylonian',\n",
       " 'babysat',\n",
       " 'babysit',\n",
       " 'babysits',\n",
       " 'babysitter',\n",
       " 'babysitters',\n",
       " 'babysitting',\n",
       " 'babyy',\n",
       " 'babyya',\n",
       " 'babyyeah',\n",
       " 'bacall',\n",
       " 'baccalauréat',\n",
       " 'baccalieri',\n",
       " 'baccarat',\n",
       " 'baccarin',\n",
       " 'bacchan',\n",
       " 'bacchus',\n",
       " 'bach',\n",
       " 'bachachan',\n",
       " 'bachan',\n",
       " 'bachar',\n",
       " 'bacharach',\n",
       " 'bachchan',\n",
       " 'bachelor',\n",
       " 'bachelorette',\n",
       " 'bachelors',\n",
       " 'bachlor',\n",
       " 'bachman',\n",
       " 'bachmann',\n",
       " 'bachrach',\n",
       " 'bachstage',\n",
       " 'bacio',\n",
       " 'back',\n",
       " 'backbeat',\n",
       " 'backbiting',\n",
       " 'backbone',\n",
       " 'backbreaking',\n",
       " 'backcountry',\n",
       " 'backdoor',\n",
       " 'backdrop',\n",
       " 'backdropped',\n",
       " 'backdrops',\n",
       " 'backed',\n",
       " 'backer',\n",
       " 'backers',\n",
       " 'backett',\n",
       " 'backfire',\n",
       " 'backfired',\n",
       " 'backfires',\n",
       " 'backflashes',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'backhanded',\n",
       " 'backing',\n",
       " 'backlash',\n",
       " 'backlashes',\n",
       " 'backlighting',\n",
       " 'backlot',\n",
       " 'backlots',\n",
       " 'backorder',\n",
       " 'backpack',\n",
       " 'backpacking',\n",
       " 'backroad',\n",
       " 'backroom',\n",
       " 'backrounds',\n",
       " 'backs',\n",
       " 'backseat',\n",
       " 'backside',\n",
       " 'backsides',\n",
       " 'backslapping',\n",
       " 'backsliding',\n",
       " 'backstabbed',\n",
       " 'backstabber',\n",
       " 'backstabbing',\n",
       " 'backstage',\n",
       " 'backstories',\n",
       " 'backstory',\n",
       " 'backstreet',\n",
       " 'backstreets',\n",
       " 'backtrack',\n",
       " 'backula',\n",
       " 'backup',\n",
       " 'backus',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'backwater',\n",
       " 'backwood',\n",
       " 'backwoods',\n",
       " 'backwoodsman',\n",
       " 'backwords',\n",
       " 'backyard',\n",
       " 'back\\x97there',\n",
       " 'bacon',\n",
       " 'bacons',\n",
       " 'bacri',\n",
       " 'bacteria',\n",
       " 'bacterial',\n",
       " 'bacterium',\n",
       " 'bacula',\n",
       " 'bad',\n",
       " 'badalamenti',\n",
       " 'badalucco',\n",
       " 'badass',\n",
       " 'badat',\n",
       " 'badd',\n",
       " 'baddddd',\n",
       " 'baddeley',\n",
       " 'badder',\n",
       " 'baddest',\n",
       " 'baddie',\n",
       " 'baddiel',\n",
       " 'baddies',\n",
       " 'baddness',\n",
       " 'baddy',\n",
       " 'bade',\n",
       " 'badel',\n",
       " 'bader',\n",
       " 'badest',\n",
       " 'badge',\n",
       " 'badged',\n",
       " 'badger',\n",
       " 'badgering',\n",
       " 'badgers',\n",
       " 'badges',\n",
       " 'badguy',\n",
       " 'badguys',\n",
       " 'badham',\n",
       " 'badie',\n",
       " 'badiel',\n",
       " 'badjatya',\n",
       " 'badjatyas',\n",
       " 'badlands',\n",
       " 'badly',\n",
       " 'badman',\n",
       " 'badmen',\n",
       " 'badminton',\n",
       " 'badmitton',\n",
       " 'badmouth',\n",
       " 'badmouthing',\n",
       " 'badnam',\n",
       " 'badness',\n",
       " 'bads',\n",
       " 'bae',\n",
       " 'baer',\n",
       " 'baez',\n",
       " 'baffel',\n",
       " 'baffeling',\n",
       " 'baffle',\n",
       " 'baffled',\n",
       " 'bafflement',\n",
       " 'baffles',\n",
       " 'baffling',\n",
       " 'bafflingly',\n",
       " 'bafta',\n",
       " 'baftas',\n",
       " 'bag',\n",
       " 'bagatelle',\n",
       " 'bagdad',\n",
       " 'bagels',\n",
       " 'baggage',\n",
       " 'bagged',\n",
       " 'bagger',\n",
       " 'bagging',\n",
       " 'baggins',\n",
       " 'bagginses',\n",
       " 'baggot',\n",
       " 'baggy',\n",
       " 'baghban',\n",
       " 'baghdad',\n",
       " 'bagheri',\n",
       " 'bagley',\n",
       " 'bagman',\n",
       " 'bagpipe',\n",
       " 'bagpipes',\n",
       " 'bags',\n",
       " 'baguette',\n",
       " 'baguettes',\n",
       " 'bah',\n",
       " 'bahal',\n",
       " 'bahamas',\n",
       " 'bahgdad',\n",
       " 'bahiyyaji',\n",
       " 'bahot',\n",
       " 'bahrain',\n",
       " 'bahumbag',\n",
       " 'bai',\n",
       " 'baichwal',\n",
       " 'bail',\n",
       " 'bailed',\n",
       " 'bailey',\n",
       " 'bailiff',\n",
       " 'bailor',\n",
       " 'bailout',\n",
       " 'bails',\n",
       " 'bailsman',\n",
       " 'baily',\n",
       " 'bain',\n",
       " 'bainter',\n",
       " 'baio',\n",
       " 'bairns',\n",
       " 'baiscally',\n",
       " 'baise',\n",
       " 'bait',\n",
       " 'baited',\n",
       " 'baiting',\n",
       " 'baits',\n",
       " 'baitz',\n",
       " 'baja',\n",
       " 'bajillion',\n",
       " 'bajo',\n",
       " 'bajpai',\n",
       " 'bak',\n",
       " 'bakalienikoff',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'bakelite',\n",
       " 'baker',\n",
       " 'bakeries',\n",
       " 'bakers',\n",
       " 'bakersfeild',\n",
       " 'bakersfield',\n",
       " 'bakery',\n",
       " 'bakes',\n",
       " 'bakesfield',\n",
       " 'bakewell',\n",
       " 'bakhtiari',\n",
       " 'bakhtyari',\n",
       " 'baking',\n",
       " 'bakke',\n",
       " 'bako',\n",
       " 'bakovic',\n",
       " 'bakshi',\n",
       " 'bakshis',\n",
       " 'bakshki',\n",
       " 'baku',\n",
       " 'bakula',\n",
       " 'bakumatsu',\n",
       " 'bakvaas',\n",
       " 'bal',\n",
       " 'bala',\n",
       " 'balaban',\n",
       " 'balaclava',\n",
       " 'balad',\n",
       " 'balaguero',\n",
       " 'balaji',\n",
       " 'balalaika',\n",
       " 'balan',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balances',\n",
       " 'balanchine',\n",
       " 'balancing',\n",
       " 'balasko',\n",
       " 'balaun',\n",
       " 'balbao',\n",
       " 'balboa',\n",
       " 'balcan',\n",
       " 'balcans',\n",
       " 'balcony',\n",
       " 'balconys',\n",
       " 'bald',\n",
       " 'balder',\n",
       " 'balderdash',\n",
       " 'balding',\n",
       " 'baldly',\n",
       " 'baldness',\n",
       " 'baldrick',\n",
       " 'baldry',\n",
       " 'balduin',\n",
       " 'baldwin',\n",
       " 'baldwins',\n",
       " 'baldy',\n",
       " 'bale',\n",
       " 'balearic',\n",
       " 'baleful',\n",
       " 'baler',\n",
       " 'bales',\n",
       " 'balfour',\n",
       " 'bali',\n",
       " 'balibar',\n",
       " 'balikbayan',\n",
       " 'balinese',\n",
       " 'balk',\n",
       " 'balkan',\n",
       " 'balkanic',\n",
       " 'balkanized',\n",
       " 'balkans',\n",
       " 'balkanski',\n",
       " 'balked',\n",
       " 'balki',\n",
       " 'balky',\n",
       " 'ball',\n",
       " 'ballad',\n",
       " 'ballads',\n",
       " 'ballantine',\n",
       " 'ballantrae',\n",
       " 'ballarat',\n",
       " 'ballard',\n",
       " 'ballast',\n",
       " 'ballbusting',\n",
       " 'balled',\n",
       " 'ballentine',\n",
       " 'baller',\n",
       " 'ballerina',\n",
       " 'ballers',\n",
       " 'ballesta',\n",
       " 'ballestra',\n",
       " 'ballet',\n",
       " 'balletic',\n",
       " 'ballets',\n",
       " 'ballgame',\n",
       " 'ballgames',\n",
       " 'ballgown',\n",
       " 'ballin',\n",
       " 'balling',\n",
       " 'ballistic',\n",
       " 'ballon',\n",
       " 'balloon',\n",
       " 'ballooned',\n",
       " 'balloonist',\n",
       " 'balloons',\n",
       " 'ballot',\n",
       " 'ballpark',\n",
       " 'ballplayer',\n",
       " 'ballplayers',\n",
       " 'ballroom',\n",
       " 'balls',\n",
       " 'ballsy',\n",
       " 'bally',\n",
       " 'ballyhoo',\n",
       " 'ballyhooed',\n",
       " 'ballykissangel',\n",
       " 'ballz',\n",
       " 'balm',\n",
       " 'balme',\n",
       " 'baloer',\n",
       " 'balois',\n",
       " 'baloney',\n",
       " 'baloo',\n",
       " 'balooned',\n",
       " 'baloons',\n",
       " 'baloopers',\n",
       " 'balraj',\n",
       " 'balrog',\n",
       " 'balsa',\n",
       " 'balsam',\n",
       " 'balsamic',\n",
       " 'balsmeyer',\n",
       " 'balta',\n",
       " 'baltar',\n",
       " 'balthasar',\n",
       " 'balthazar',\n",
       " 'baltic',\n",
       " 'baltimore',\n",
       " 'baltimorean',\n",
       " 'baltimoreans',\n",
       " 'baltron',\n",
       " 'baltz',\n",
       " 'balu',\n",
       " 'balwin',\n",
       " 'balzac',\n",
       " 'bam',\n",
       " 'bama',\n",
       " 'bamatabois',\n",
       " 'bamba',\n",
       " 'bambaata',\n",
       " 'bambaataa',\n",
       " 'bambaiya',\n",
       " 'bamber',\n",
       " 'bambi',\n",
       " 'bambies',\n",
       " 'bambini',\n",
       " 'bambino',\n",
       " 'bamboo',\n",
       " 'bamboozled',\n",
       " 'bamboozles',\n",
       " 'bamboozling',\n",
       " 'bambou',\n",
       " 'bamrha',\n",
       " 'bams',\n",
       " 'ban',\n",
       " 'bana',\n",
       " 'banal',\n",
       " 'banalities',\n",
       " 'banality',\n",
       " 'banally',\n",
       " 'banana',\n",
       " 'bananaman',\n",
       " 'bananas',\n",
       " 'banaras',\n",
       " 'banco',\n",
       " 'bancroft',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandaged',\n",
       " 'bandages',\n",
       " 'bandai',\n",
       " 'bandalier',\n",
       " 'bandannas',\n",
       " 'bandaras',\n",
       " 'bandaur',\n",
       " 'banded',\n",
       " 'bandekar',\n",
       " 'banderas',\n",
       " 'bandes',\n",
       " 'bandido',\n",
       " 'bandied',\n",
       " 'banding',\n",
       " 'bandini',\n",
       " 'bandit',\n",
       " 'bandits',\n",
       " 'bandmates',\n",
       " 'bando',\n",
       " 'bandolero',\n",
       " 'bands',\n",
       " 'bandstand',\n",
       " 'bandwagon',\n",
       " 'bandwidth',\n",
       " 'bandy',\n",
       " 'bane',\n",
       " 'bang',\n",
       " 'banged',\n",
       " 'bangers',\n",
       " 'banging',\n",
       " 'bangkok',\n",
       " 'bangla',\n",
       " 'bangladesh',\n",
       " 'bangs',\n",
       " 'bangster',\n",
       " 'bangville',\n",
       " 'bania',\n",
       " 'banish',\n",
       " 'banished',\n",
       " 'banishing',\n",
       " 'banishses',\n",
       " 'banister',\n",
       " 'banjo',\n",
       " 'banjoes',\n",
       " 'banjos',\n",
       " 'bank',\n",
       " 'bankability',\n",
       " 'bankable',\n",
       " 'bankcrupcy',\n",
       " 'banked',\n",
       " 'banker',\n",
       " 'bankers',\n",
       " 'banking',\n",
       " 'bankolé',\n",
       " 'bankrobber',\n",
       " 'bankrobbers',\n",
       " 'bankroll',\n",
       " 'bankrolled',\n",
       " 'bankrolling',\n",
       " 'bankrolls',\n",
       " 'bankrupt',\n",
       " 'bankruptcy',\n",
       " 'bankrupted',\n",
       " 'banks',\n",
       " 'bannacheck',\n",
       " 'banned',\n",
       " 'bannen',\n",
       " 'banner',\n",
       " 'banners',\n",
       " 'banning',\n",
       " 'bannings',\n",
       " 'bannister',\n",
       " 'bannon',\n",
       " 'banquet',\n",
       " 'banquo',\n",
       " 'bans',\n",
       " 'banshee',\n",
       " 'banshees',\n",
       " 'bansihed',\n",
       " 'bantam',\n",
       " 'banter',\n",
       " 'bantering',\n",
       " 'banters',\n",
       " 'banter\\x97even',\n",
       " 'bantha',\n",
       " 'banton',\n",
       " 'banu',\n",
       " 'banyo',\n",
       " 'banzai',\n",
       " 'bao',\n",
       " 'baokar',\n",
       " 'baphomets',\n",
       " 'baps',\n",
       " 'baptised',\n",
       " 'baptises',\n",
       " 'baptism',\n",
       " 'baptist',\n",
       " 'baptiste',\n",
       " 'baptists',\n",
       " 'baptized',\n",
       " 'bapu',\n",
       " 'bar',\n",
       " 'bara',\n",
       " 'barabar',\n",
       " 'barabarian',\n",
       " 'barabas',\n",
       " 'barabra',\n",
       " 'barack',\n",
       " 'baragrey',\n",
       " 'baraka',\n",
       " 'baranov',\n",
       " 'baranski',\n",
       " 'baransky',\n",
       " 'barantini',\n",
       " 'baras',\n",
       " 'barataria',\n",
       " 'barb',\n",
       " 'barbados',\n",
       " 'barbapapa',\n",
       " 'barbara',\n",
       " 'barbarash',\n",
       " 'barbarella',\n",
       " 'barbarellish',\n",
       " 'barbarian',\n",
       " 'barbarians',\n",
       " 'barbaric',\n",
       " 'barbarically',\n",
       " 'barbarino',\n",
       " 'barbarism',\n",
       " 'barbarossa',\n",
       " 'barbarous',\n",
       " 'barbeau',\n",
       " 'barbecue',\n",
       " 'barbecued',\n",
       " 'barbed',\n",
       " 'barbedwire',\n",
       " 'barbells',\n",
       " 'barber',\n",
       " 'barbera',\n",
       " 'barbers',\n",
       " 'barbershop',\n",
       " 'barbet',\n",
       " 'barbi',\n",
       " 'barbie',\n",
       " 'barbies',\n",
       " 'barbirino',\n",
       " 'barboo',\n",
       " 'barbour',\n",
       " 'barbra',\n",
       " 'barbs',\n",
       " 'barbu',\n",
       " 'barbwire',\n",
       " 'barc',\n",
       " 'barca',\n",
       " 'barcelona',\n",
       " 'barcelonans',\n",
       " 'barclay',\n",
       " 'barcode',\n",
       " 'barcoded',\n",
       " 'bard',\n",
       " 'bardeleben',\n",
       " 'bardem',\n",
       " 'bardot',\n",
       " 'bards',\n",
       " 'bardwork',\n",
       " 'bare',\n",
       " 'barebones',\n",
       " 'bared',\n",
       " 'barefaced',\n",
       " 'barefoot',\n",
       " 'barek',\n",
       " 'barely',\n",
       " 'barem',\n",
       " 'bares',\n",
       " 'barest',\n",
       " 'baretta',\n",
       " 'barf',\n",
       " 'barfed',\n",
       " 'barfing',\n",
       " 'barfly',\n",
       " 'barfuss',\n",
       " 'bargain',\n",
       " 'bargained',\n",
       " 'bargaining',\n",
       " 'bargains',\n",
       " 'barge',\n",
       " 'bargearse',\n",
       " 'bargepoles',\n",
       " 'barger',\n",
       " 'bargin',\n",
       " 'barging',\n",
       " 'bargo',\n",
       " 'baring',\n",
       " 'barings',\n",
       " 'barish',\n",
       " 'baritone',\n",
       " 'barjatya',\n",
       " 'barjatyagot',\n",
       " 'barjatyas',\n",
       " 'bark',\n",
       " 'barker',\n",
       " 'barkers',\n",
       " 'barkin',\n",
       " 'barking',\n",
       " 'barkley',\n",
       " 'barks',\n",
       " 'barky',\n",
       " 'barley',\n",
       " 'barlow',\n",
       " 'barmaid',\n",
       " 'barmaids',\n",
       " 'barman',\n",
       " 'barmans',\n",
       " 'barmen',\n",
       " 'barmitzvah',\n",
       " 'barmy',\n",
       " 'barn',\n",
       " 'barnabus',\n",
       " 'barnaby',\n",
       " 'barnacle',\n",
       " 'barnacles',\n",
       " 'barnard',\n",
       " 'barnes',\n",
       " 'barnet',\n",
       " 'barnett',\n",
       " 'barney',\n",
       " 'barnstorming',\n",
       " 'barnum',\n",
       " 'barnwell',\n",
       " 'barnyard',\n",
       " 'barometer',\n",
       " 'barometers',\n",
       " 'barometric',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[5000:50005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are splitting the text document 1 from the training set based on space (' ')and taking all the lowercase \n",
    "words and their length is almost same as the one calculated in document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"all\".',\n",
       " '\"big',\n",
       " '\"mark',\n",
       " '\"ten',\n",
       " '\"the',\n",
       " '\"witchery\"',\n",
       " '(a',\n",
       " '(and',\n",
       " '(i',\n",
       " '(like',\n",
       " '-',\n",
       " '/><br',\n",
       " '/>and',\n",
       " '/>but',\n",
       " \"/>don't\",\n",
       " '/>from',\n",
       " '/>no',\n",
       " '/>no,',\n",
       " '/>okay,',\n",
       " '/>who',\n",
       " '<br',\n",
       " 'a',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'apart',\n",
       " 'are',\n",
       " 'as',\n",
       " 'back',\n",
       " 'band.<br',\n",
       " 'beack',\n",
       " 'bear',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'benefit',\n",
       " 'blair',\n",
       " 'blame',\n",
       " 'bother.',\n",
       " 'bottom',\n",
       " 'bursting',\n",
       " 'by',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cast\"',\n",
       " 'celebrity',\n",
       " 'center',\n",
       " 'child',\n",
       " 'cliches.',\n",
       " 'coming',\n",
       " 'compost',\n",
       " 'count',\n",
       " 'cringe',\n",
       " 'cross-section',\n",
       " 'cult',\n",
       " 'dangled',\n",
       " 'david',\n",
       " 'devil\"',\n",
       " 'did.',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'dunno,',\n",
       " 'during',\n",
       " 'eastern',\n",
       " 'ecchhhh.',\n",
       " 'effervescent',\n",
       " 'end',\n",
       " 'entertaining',\n",
       " 'equal',\n",
       " 'etc,',\n",
       " 'etc.',\n",
       " 'even',\n",
       " 'exorcist\",',\n",
       " 'falls',\n",
       " 'faster',\n",
       " 'few',\n",
       " 'film,',\n",
       " 'find',\n",
       " 'fireplaces,',\n",
       " 'folks',\n",
       " 'followed',\n",
       " 'for',\n",
       " 'from',\n",
       " 'get',\n",
       " 'gets',\n",
       " \"gettin'\",\n",
       " 'give',\n",
       " 'giving',\n",
       " 'golf',\n",
       " 'grotesque,',\n",
       " 'guess)',\n",
       " 'hasselhoff',\n",
       " 'have',\n",
       " 'he',\n",
       " 'heap.<br',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'hildegard',\n",
       " 'him?)',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'm\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'indians\"',\n",
       " 'inside',\n",
       " 'intent',\n",
       " 'interesting',\n",
       " 'is',\n",
       " 'island',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'it,',\n",
       " 'itches.',\n",
       " 'just',\n",
       " 'knef',\n",
       " 'knows:',\n",
       " 'knows;',\n",
       " 'lesser',\n",
       " 'likes',\n",
       " 'linda',\n",
       " 'linda...',\n",
       " 'line.',\n",
       " 'little',\n",
       " 'looks',\n",
       " 'main',\n",
       " 'mansion',\n",
       " 'marching',\n",
       " 'matters',\n",
       " 'may',\n",
       " 'most',\n",
       " 'mouths',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'much',\n",
       " 'much.',\n",
       " 'name',\n",
       " 'necks,',\n",
       " 'none',\n",
       " 'not',\n",
       " 'nowadays.<br',\n",
       " 'nudge-nudge)',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offed',\n",
       " 'omen\",',\n",
       " 'on',\n",
       " 'one',\n",
       " 'one-line',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'out',\n",
       " 'over.<br',\n",
       " 'pam',\n",
       " 'papier-mache',\n",
       " 'part,',\n",
       " 'paycheck',\n",
       " 'people',\n",
       " 'peta',\n",
       " 'pick',\n",
       " 'pitch',\n",
       " 'plots',\n",
       " 'possessed,',\n",
       " 'pro-am',\n",
       " 'probably',\n",
       " 'rainstorm.',\n",
       " 'right',\n",
       " 'roaring',\n",
       " 'sadistic',\n",
       " 'satanic',\n",
       " 'satanic-worshippers-run-amok',\n",
       " 'scene',\n",
       " 'scenes!',\n",
       " 'sculpture',\n",
       " 'seaboard,',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seems',\n",
       " 'seen....<br',\n",
       " 'sewn',\n",
       " \"she's\",\n",
       " 'sheesh,',\n",
       " 'shut,',\n",
       " 'sick',\n",
       " 'some',\n",
       " 'sound',\n",
       " 'sounded',\n",
       " 'stars.',\n",
       " 'storm,',\n",
       " 'street',\n",
       " 'strung',\n",
       " 'studios,',\n",
       " 'synopsis',\n",
       " 'targeted',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " \"they're\",\n",
       " 'thought',\n",
       " 'time.',\n",
       " 'to',\n",
       " 'told',\n",
       " 'tortuous',\n",
       " 'torture',\n",
       " 'tour',\n",
       " 'trapped',\n",
       " 'trees',\n",
       " 'unfortunate',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'vessels',\n",
       " 'violent',\n",
       " 'was',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'ways.',\n",
       " 'well,',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'while',\n",
       " 'who',\n",
       " 'will',\n",
       " 'wink-wink,',\n",
       " 'wishes',\n",
       " 'witches',\n",
       " 'with',\n",
       " 'woman,',\n",
       " 'woman...).<br',\n",
       " 'worth',\n",
       " 'yes,',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = set([o.lower() for o in trn[0].split(' ')])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.vocabulary_['absurd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function will give the column number of the word in document matrix. Document matrix doesn't care about the \n",
    "relative ordering of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log count ratio r for each word f is log(ratio of that word in +ve doc/ratio of that word in -ve doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trn_doc\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn_y\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have grabs all the rows with y==1 and then sum it and plus 1. Similarly for the y==0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = x[y==1].sum(0)+1\n",
    "q = x[y==0].sum(0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[    2,     1, 11820, ...,     2,     2,     1]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 75132)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[    1,     2, 12742, ...,     1,     2,     8]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 75132)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log((p/p.sum())/(q/q.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.68213, -0.70417, -0.08613, ...,  0.68213, -0.01102, -2.09046]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 75132)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So r is the log count ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.log(len(p)/len(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Naive Bayes Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_doc @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the binarized version of Naive Bayes. In this .sign() finction will convert the anything >0 with 1 and <0 with\n",
    "-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82624"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_doc.sign() @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual parameter will make the Logistic Regression run faster on Larger Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanshu/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/priyanshu/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85768"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Binarized version of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanshu/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85488"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(trn_doc.sign(), y)\n",
    "preds = m.predict(val_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with huge number of parameter, so there is a chance that our model can overfit. So we have to \n",
    "regularized it. We can make the use of C parameter in order to regularized the data. More C value, less regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanshu/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88272"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(trn_doc, y)\n",
    "preds = m.predict(val_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88404"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(trn_doc.sign(), y)\n",
    "preds = m.predict(val_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy is another loss function used for classification purpose. \n",
    "Formula is CE = act_y*log(pre_y)-(1-act_y)*log(1-pre_y), where act_y = actual classification and pre_y = predicted \n",
    "classificatiom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification purpose use Cross Entropy as a loss function and for Regression use RMSE loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram with NB Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to add more features by adding digram and trigram to our dataset. So we are taking only top 800000 features into consideration and then adding the features to the document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_doc = vec.fit_transform(trn)\n",
    "val_doc = vec.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_doc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting some of the features from 800000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200000:200005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn_y\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all the value > 0 to 1 and all the value < 0 to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x800000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12589101 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trn_doc.sign()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x800000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11555871 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x = val_doc.sign()\n",
    "val_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now applying Naive Bayes to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = x[y==1].sum(0)+1\n",
    "q = x[y==0].sum(0)+1\n",
    "r = np.log((p/p.sum())/(q/q.sum()))\n",
    "b = np.log(len(p)/len(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88044"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_x @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now applying logistic regression on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanshu/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x,y)\n",
    "preds = m.predict(val_x)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 800000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.04911, -0.15543, -0.24226, ...,  1.10419, -0.68757, -0.68757]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.95208, 0.85605, 0.78485, ..., 3.01678, 0.5028 , 0.5028 ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r is actually the log value of ratios so to get the actual ratio we need to do e^(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyanshu/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91768"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x_nb,y)\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why we are getting better accuracy on mulitpling the document metrix with r?? This is because initially for the \n",
    "negative document we have 0 and for positive we have 1 but after multipling it by r our value for negative document is\n",
    "0.3333 and for positive we have 3. So indirectly we have increase the amount of variance between the positive and \n",
    "negative document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai NBSVM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is amount of unique we have to use for the further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = TextClassifierData.from_bow(trn_doc, trn_y, val_doc, val_y, sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are getting the model from bag of words. Here md is data object which wraps both training and validation data\n",
    "and this bag of words will have only 2000 unique words for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = md.dotprod_nb_learner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dotprod_nd_learner() function is NBSVM for the fastai library. Now we are going to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04108e0819834e1db42feaa50f40b728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.025744   0.11924    0.91664   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11924]), 0.9166400000190735]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fde2bfa80744ff828af9c1d432733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.021512   0.113841   0.91992   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11384]), 0.9199199999809266]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15b4fdcaac740dd909784425be7a583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.018895   0.111163   0.92208   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11116]), 0.9220800000190735]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ee957d62414b5fadd92b09d92acaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.017486   0.110443   0.92172   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11044]), 0.9217199999809265]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09a1811d3d442d899aed269bd554f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.016829   0.109381   0.92244   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.10938]), 0.9224399999809265]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after 5 epoch we are getting the accuracy of around 92.2% which is quite better then previous versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in dotprod_nb_learner() function we add some constant to the wx this is beacuse regularization tries to make \n",
    "everything zero. And if everything is zero then complete matrix multiplication will be zero. So to avoid this condition\n",
    "we usually add some constant most probably 0.4 to the wx, so that wx doesn't reduce to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
